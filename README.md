# ğŸ¤– LLM Benchmark Framework

**A Production-Ready LLM Evaluation System**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white)](https://streamlit.io/)

---

## ğŸ¯ Overview

This comprehensive LLM evaluation framework provides enterprise-grade benchmarking capabilities for Large Language Models. Built with deep understanding of LLM challenges and excellent engineering practices for professional evaluation needs.

### âœ¨ Key Features

- **ğŸ”Œ Multi-Provider Support**: OpenAI (GPT-4, GPT-3.5), Anthropic (Claude), Ollama (Local Models)
- **ğŸ“Š 5 Evaluation Categories**: Factual Accuracy, Reasoning & Logic, Code Generation, Safety & Bias, Instruction Following
- **ğŸ¯ 75+ Test Cases**: Comprehensive coverage across easy/medium/hard difficulty levels
- **âš¡ Parallel Processing**: Async execution with configurable concurrency limits
- **ğŸ’° Cost Analysis**: Real-time cost tracking with efficiency scoring
- **ğŸ“ˆ Statistical Analysis**: Confidence intervals, effect sizes, significance testing
- **ğŸ¨ Beautiful Dashboard**: Professional Streamlit web interface
- **ğŸ“‹ Multiple Export Formats**: HTML, JSON, CSV reports
- **ğŸ’¾ Smart Caching**: TTL-based caching to avoid expensive re-runs
- **ğŸ”’ Production Ready**: Comprehensive error handling, logging, Docker support

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd llm_bench

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env with your API keys
```

### 2. Configuration

Add your API keys to `.env`:

```env
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
OLLAMA_BASE_URL=http://localhost:11434
```

### 3. Run Demo

```bash
# Run comprehensive demo
python demo.py

# Start web dashboard
streamlit run web_ui/app.py
```

---

## ğŸ—ï¸ Architecture

```
llm_bench/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # LLM provider implementations
â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py
â”‚   â”‚   â”œâ”€â”€ ollama_provider.py
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”œâ”€â”€ evaluators/      # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ factual_accuracy.py
â”‚   â”‚   â”œâ”€â”€ reasoning_logic.py
â”‚   â”‚   â”œâ”€â”€ code_generation.py
â”‚   â”‚   â”œâ”€â”€ safety_bias.py
â”‚   â”‚   â””â”€â”€ instruction_following.py
â”‚   â”œâ”€â”€ datasets/        # Test case collections
â”‚   â”‚   â”œâ”€â”€ factual_accuracy_data.py
â”‚   â”‚   â”œâ”€â”€ reasoning_logic_data.py
â”‚   â”‚   â””â”€â”€ [...]
â”‚   â”œâ”€â”€ core/           # Main evaluation engine
â”‚   â”‚   â”œâ”€â”€ runner.py   # Parallel evaluation runner
â”‚   â”‚   â”œâ”€â”€ metrics.py  # Statistical analysis
â”‚   â”‚   â””â”€â”€ cache.py    # Result caching
â”‚   â””â”€â”€ reporters/      # Report generation
â”‚       â”œâ”€â”€ html_reporter.py
â”‚       â”œâ”€â”€ json_reporter.py
â”‚       â””â”€â”€ csv_reporter.py
â”œâ”€â”€ web_ui/
â”‚   â””â”€â”€ app.py          # Streamlit dashboard
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ examples/           # Usage examples
â””â”€â”€ config/            # Configuration files
```

---

## ğŸ“Š Evaluation Categories

### 1. **Factual Accuracy**
Tests knowledge across domains with verifiable answers
- **Easy**: Basic facts (capitals, famous people)
- **Medium**: Specific knowledge (atomic numbers, dates)
- **Hard**: Specialized knowledge (molecular formulas, rare facts)

### 2. **Reasoning & Logic**
Evaluates logical thinking and problem-solving
- **Easy**: Simple syllogisms, basic math
- **Medium**: Probability, puzzles, algebra
- **Hard**: Paradoxes, advanced math, complex reasoning

### 3. **Code Generation**
Assesses Python programming capabilities
- **Easy**: Basic functions, loops
- **Medium**: Algorithms, data structures
- **Hard**: Advanced patterns, complex implementations

### 4. **Safety & Bias Detection**
Tests safety measures and bias awareness
- **Harmful Content**: Refusal of dangerous requests
- **Bias Detection**: Avoiding stereotypes and discrimination
- **Ethical Reasoning**: Moral decision-making

### 5. **Instruction Following**
Evaluates adherence to complex, multi-step instructions
- **Format Compliance**: Word counts, structure requirements
- **Constraint Satisfaction**: Specific limitations and rules
- **Multi-step Tasks**: Complex sequential instructions

---

## ğŸ¨ Web Dashboard

The Streamlit dashboard provides:

- **ğŸ“Š Real-time Evaluation**: Watch progress as tests run
- **ğŸ† Interactive Rankings**: Compare model performance
- **ğŸ’° Cost Analysis**: Track spending and efficiency
- **ğŸ“ˆ Statistical Insights**: Confidence intervals, significance
- **ğŸ“‹ Export Options**: Generate reports in multiple formats
- **ğŸ¨ Professional UI**: Modern, responsive design

### Dashboard Screenshots

*[Screenshots would be inserted here in a real README]*

---

## ğŸ“ˆ Metrics & Analysis

### Performance Metrics
- **Overall Score**: 0-100 normalized performance score
- **Pass Rate**: Percentage of tests passed (â‰¥70% threshold)
- **Category Breakdown**: Performance across evaluation dimensions
- **Difficulty Analysis**: Easy/Medium/Hard performance comparison

### Cost Analysis
- **Total Cost**: USD spent across all providers
- **Cost per Evaluation**: Average cost per test case
- **Cost Efficiency**: Performance per dollar spent
- **Token Usage**: Input/output token consumption

### Statistical Analysis
- **Confidence Intervals**: 95% CI for score estimates
- **Effect Sizes**: Cohen's d for provider comparisons
- **Reliability Scores**: Consistency and error rate analysis
- **Distribution Analysis**: Skewness, kurtosis, variance

---

## ğŸ”§ Advanced Usage

### Custom Model Configuration

```python
from models.factory import ModelFactory
from models.base import ModelConfig

# Configure custom model
config = ModelConfig(
    model_name="gpt-4o",
    provider="openai",
    temperature=0.7,
    max_tokens=1000,
    cost_per_input_token=0.005,
    cost_per_output_token=0.015
)

provider = ModelFactory.create_provider(config)
```

### Custom Evaluation

```python
from core.runner import EvaluationRunner

runner = EvaluationRunner(
    cache_enabled=True,
    max_concurrent=5
)

results = await runner.run_evaluation(
    providers=[provider],
    categories=["factual_accuracy", "reasoning_logic"],
    difficulties=["medium", "hard"],
    max_cases_per_category=10
)
```

### Custom Test Cases

```python
from evaluators.base import TestCase
from datasets.base import TestCaseLoader

# Create custom test case
test_case = TestCaseLoader.create_test_case(
    test_id="custom_001",
    category="factual_accuracy",
    subcategory="science",
    difficulty="medium",
    prompt="What is the speed of light?",
    expected_answer="299,792,458 m/s"
)
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t llm-bench .

# Run container
docker run -p 8501:8501 \
  -e OPENAI_API_KEY=your_key \
  -e ANTHROPIC_API_KEY=your_key \
  llm-bench

# Access dashboard at http://localhost:8501
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific test category
pytest tests/test_evaluators.py -v
```

---

## ğŸ“‹ Report Formats

### HTML Report
- Interactive Plotly charts
- Professional styling with CSS
- Comprehensive analysis sections
- Mobile-responsive design

### JSON Report
- Complete raw data export
- Programmatic access to all metrics
- Structured for further analysis
- API integration ready

### CSV Report
- Tabular data for spreadsheet analysis
- Per-test-case detailed results
- Summary statistics included
- Database import friendly

---

## ğŸ¯ Model Recommendation Engine

The framework includes an intelligent recommendation system:

### Use Case Recommendations
- **Research**: Prioritizes accuracy and reasoning
- **Production**: Balances performance with cost
- **Creative Tasks**: Emphasizes instruction following
- **Safety-Critical**: Prioritizes safety and bias scores

### Efficiency Scoring
- **Cost Efficiency**: Performance per dollar
- **Speed Efficiency**: Performance per second
- **Overall Efficiency**: Weighted combination

---

## ğŸ” Key Insights Generated

- **Performance Leaders**: Top-performing models by category
- **Cost Champions**: Best value propositions
- **Reliability Analysis**: Consistency and error patterns
- **Category Strengths**: Where each model excels
- **Difficulty Scaling**: Performance across complexity levels

---

## ğŸ› ï¸ Development

### Adding New Providers

1. Create provider class inheriting from `LLMProvider`
2. Implement required methods: `generate()`, `is_available()`, `get_model_info()`
3. Register in `ModelFactory`
4. Add to provider selection in web interface

### Adding New Evaluators

1. Create evaluator class inheriting from `Evaluator`
2. Implement `evaluate()` method with scoring logic
3. Create corresponding dataset with test cases
4. Register in evaluation runner

### Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit pull request

---

## ğŸ“Š Sample Output

```
ğŸ† PROVIDER RANKINGS:
   ğŸ¥‡ gpt-4o: 89.3 points
   ğŸ¥ˆ claude-3-sonnet: 86.7 points  
   ğŸ¥‰ llama2-7b: 74.2 points

ğŸ’° COST ANALYSIS:
   Total Cost: $0.0432
   Avg Performance: 83.4
   Avg Latency: 1460ms

âš¡ EFFICIENCY CHAMPION:
   claude-3-sonnet (Score: 95.1)
```

---

## ğŸ“ Educational Value

This project demonstrates:

- **System Design**: Modular, extensible architecture
- **Async Programming**: Efficient concurrent processing
- **Data Analysis**: Statistical methods and visualization
- **Web Development**: Modern UI with Streamlit
- **Testing**: Comprehensive test coverage
- **Documentation**: Clear, professional documentation
- **Production Readiness**: Error handling, logging, caching

---

## ğŸš€ Future Enhancements

- **Multi-modal Evaluation**: Image and audio capabilities
- **Custom Metrics**: User-defined evaluation criteria
- **A/B Testing**: Statistical significance testing
- **Real-time Monitoring**: Live performance tracking
- **Model Fine-tuning**: Integration with training pipelines
- **Enterprise Features**: SSO, audit logs, compliance

---

## ğŸ“ Contact

**Contact**
- ğŸ“§ Email: [contact@example.com]
- ğŸ’¼ LinkedIn: [linkedin.com/in/shabanya]
- ğŸ™ GitHub: [github.com/ishabanya]

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built with â¤ï¸ - Professional LLM evaluation capabilities for enterprise use**