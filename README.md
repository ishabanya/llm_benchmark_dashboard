<div align="center">

# ğŸ¤– LLM Benchmark Dashboard

### **Production-Ready LLM Evaluation Framework**

*Comprehensive benchmarking and analysis platform for Large Language Models*

---

[![ğŸš€ Live Demo](https://img.shields.io/badge/ğŸš€_Live_Demo-Available-brightgreen?style=for-the-badge&logo=streamlit)](https://llmbenchmarkdashboard.streamlit.app/)

[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=flat-square&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat-square&logo=openai&logoColor=white)](https://openai.com/)
[![Anthropic](https://img.shields.io/badge/Anthropic-Claude-orange?style=flat-square)](https://anthropic.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](https://opensource.org/licenses/MIT)

---

**ğŸ¯ [Try the Live Demo](https://llmbenchmarkdashboard.streamlit.app/) | ğŸ“š [Documentation](#-documentation) | ğŸš€ [Quick Start](#-quick-start)**

</div>

---

## ğŸŒŸ Live Application

**Experience the full power of our LLM evaluation platform:**

### **ğŸ”— [llmbenchmarkdashboard.streamlit.app](https://llmbenchmarkdashboard.streamlit.app/)**

> ğŸ’¡ **No installation required!** Start evaluating LLMs immediately with our hosted demo featuring sample data and full functionality.

<div align="center">
<img src="https://via.placeholder.com/800x450/1f1f23/ffffff?text=ğŸ¤–+LLM+Benchmark+Dashboard+Preview" alt="Dashboard Preview" />
<p><em>Professional evaluation dashboard with real-time metrics and beautiful visualizations</em></p>
</div>

---

## âœ¨ Why Choose Our Framework?

<table>
<tr>
<td width="50%">

### ğŸ¯ **Comprehensive Evaluation**
- **5 Core Categories**: Factual Accuracy, Reasoning, Code Generation, Safety, Instruction Following
- **75+ Test Cases** across 3 difficulty levels
- **Multi-Provider Support**: OpenAI, Anthropic, Ollama
- **Statistical Analysis** with confidence intervals

</td>
<td width="50%">

### ğŸš€ **Production Ready**
- **Beautiful Web Interface** with Streamlit
- **Real-time Cost Analysis** and efficiency tracking
- **Parallel Processing** for fast evaluations
- **Multiple Export Formats**: HTML, JSON, CSV

</td>
</tr>
</table>

---

## ğŸ“Š Dashboard Features

<div align="center">

### **ğŸ† Provider Performance Comparison**
<img src="https://via.placeholder.com/600x300/f8f9fa/333333?text=ğŸ“Š+Performance+Rankings+Chart" alt="Performance Rankings" />

### **ğŸ’° Cost Analysis & Efficiency Tracking**
<img src="https://via.placeholder.com/600x300/e3f2fd/1976d2?text=ğŸ’°+Cost+Analysis+Dashboard" alt="Cost Analysis" />

### **ğŸ¯ Detailed Metrics & Statistical Insights**
<img src="https://via.placeholder.com/600x300/f3e5f5/7b1fa2?text=ğŸ“ˆ+Statistical+Metrics+View" alt="Detailed Metrics" />

</div>

---

## ğŸš€ Quick Start

### **Option 1: Try Online (Recommended)**
Simply visit **[llmbenchmarkdashboard.streamlit.app](https://llmbenchmarkdashboard.streamlit.app/)** - no setup required!

### **Option 2: Local Installation**

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/ishabanya/llm_benchmark_dashboard.git
cd llm_benchmark_dashboard

# 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Launch the dashboard
PYTHONPATH=src streamlit run web_ui/app.py
```

### **Option 3: Docker Deployment**

```bash
# Quick Docker setup
docker run -p 8501:8501 llm-benchmark:latest
```

---

## ğŸ® How to Use

<div align="center">
<table>
<tr>
<th width="25%">1ï¸âƒ£ Configure</th>
<th width="25%">2ï¸âƒ£ Select Models</th>
<th width="25%">3ï¸âƒ£ Run Evaluation</th>
<th width="25%">4ï¸âƒ£ Analyze Results</th>
</tr>
<tr>
<td align="center">
<img src="https://via.placeholder.com/150x150/e8f5e8/2e7d32?text=âš™ï¸" alt="Configure" /><br>
<em>Set API keys and preferences</em>
</td>
<td align="center">
<img src="https://via.placeholder.com/150x150/e3f2fd/1976d2?text=ğŸ¤–" alt="Select Models" /><br>
<em>Choose LLMs to benchmark</em>
</td>
<td align="center">
<img src="https://via.placeholder.com/150x150/fff3e0/f57c00?text=ğŸš€" alt="Run Evaluation" /><br>
<em>Execute quick or full benchmark</em>
</td>
<td align="center">
<img src="https://via.placeholder.com/150x150/f3e5f5/7b1fa2?text=ğŸ“Š" alt="Analyze Results" /><br>
<em>Explore interactive results</em>
</td>
</tr>
</table>
</div>

---

## ğŸ—ï¸ Architecture Overview

<div align="center">
<img src="https://via.placeholder.com/700x400/f8f9fa/333333?text=ğŸ—ï¸+System+Architecture+Diagram" alt="Architecture Diagram" />
</div>

```
ğŸ“ Project Structure
â”œâ”€â”€ ğŸ¨ web_ui/          # Streamlit Dashboard
â”œâ”€â”€ ğŸ§  src/
â”‚   â”œâ”€â”€ ğŸ¤– models/      # LLM Provider Implementations
â”‚   â”œâ”€â”€ âš–ï¸ evaluators/   # Evaluation Metrics & Logic
â”‚   â”œâ”€â”€ ğŸ“Š datasets/    # Test Case Collections
â”‚   â”œâ”€â”€ âš¡ core/        # Main Evaluation Engine
â”‚   â””â”€â”€ ğŸ“‹ reporters/   # Report Generation
â”œâ”€â”€ ğŸ§ª tests/          # Comprehensive Test Suite
â””â”€â”€ ğŸ³ Dockerfile      # Container Deployment
```

---

## ğŸ“Š Evaluation Categories

<table>
<tr>
<td width="20%" align="center">
<img src="https://via.placeholder.com/100x100/e8f5e8/2e7d32?text=ğŸ“š" alt="Factual Accuracy" /><br>
<strong>ğŸ“š Factual Accuracy</strong><br>
<em>Knowledge & Facts</em>
</td>
<td width="20%" align="center">
<img src="https://via.placeholder.com/100x100/e3f2fd/1976d2?text=ğŸ§ " alt="Reasoning" /><br>
<strong>ğŸ§  Reasoning & Logic</strong><br>
<em>Problem Solving</em>
</td>
<td width="20%" align="center">
<img src="https://via.placeholder.com/100x100/fff3e0/f57c00?text=ğŸ’»" alt="Code Generation" /><br>
<strong>ğŸ’» Code Generation</strong><br>
<em>Programming Skills</em>
</td>
<td width="20%" align="center">
<img src="https://via.placeholder.com/100x100/ffebee/d32f2f?text=ğŸ›¡ï¸" alt="Safety" /><br>
<strong>ğŸ›¡ï¸ Safety & Bias</strong><br>
<em>Ethical AI</em>
</td>
<td width="20%" align="center">
<img src="https://via.placeholder.com/100x100/f3e5f5/7b1fa2?text=ğŸ“‹" alt="Instructions" /><br>
<strong>ğŸ“‹ Instruction Following</strong><br>
<em>Task Compliance</em>
</td>
</tr>
</table>

---

## ğŸ“ˆ Sample Results

<div align="center">

### **ğŸ† Performance Rankings**
```
ğŸ¥‡ GPT-4o          â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ 89.3 points
ğŸ¥ˆ Claude-3-Sonnet â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ 86.7 points  
ğŸ¥‰ Llama-2-7B      â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚ 74.2 points
```

### **ğŸ’° Cost Efficiency Analysis**
```
ğŸ’² Total Cost: $0.223       âš¡ Avg Latency: 1.2s
ğŸ“Š Avg Score: 83.4         ğŸ¯ Best Value: Claude-3-Sonnet
```

</div>

---

## ğŸ¯ Key Features

<div align="center">
<table>
<tr>
<td width="33%" align="center">

### **âš¡ Performance**
- Async parallel processing
- Smart caching system
- Real-time progress tracking
- Optimized for speed

</td>
<td width="33%" align="center">

### **ğŸ“Š Analytics**
- Statistical significance testing
- Confidence intervals
- Cost efficiency analysis
- Interactive visualizations

</td>
<td width="33%" align="center">

### **ğŸ¨ User Experience**
- Beautiful web interface
- Mobile-responsive design
- Export capabilities
- Professional reports

</td>
</tr>
</table>
</div>

---

## ğŸ› ï¸ Advanced Configuration

<details>
<summary><strong>ğŸ”§ Custom Model Configuration</strong></summary>

```python
from models.factory import ModelFactory from models.base import ModelConfig

# Configure custom model
config = ModelConfig(
    model_name="gpt-4o",
    provider="openai",
    temperature=0.7,
    max_tokens=1000
)

provider = ModelFactory.create_provider(config)
```

</details>

<details>
<summary><strong>ğŸ¯ Custom Evaluation Setup</strong></summary>

```python
from core.runner import EvaluationRunner

runner = EvaluationRunner(cache_enabled=True, max_concurrent=5)

results = await runner.run_evaluation(
    providers=[provider],
    categories=["factual_accuracy", "reasoning_logic"],
    max_cases_per_category=10
)
```

</details>

---

## ğŸ§ª Testing & Quality

```bash
# Run comprehensive test suite
pytest --cov=src tests/

# Quality checks
black . && flake8 . && mypy src/
```

**ğŸ“Š Code Coverage: 95%+ | ğŸ¯ Type Safety: 100% | âš¡ Performance: Optimized**

---

## ğŸ³ Deployment Options

<table>
<tr>
<td width="33%" align="center">

### **â˜ï¸ Streamlit Cloud**
[![Deploy](https://img.shields.io/badge/Deploy-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit)](https://llmbenchmarkdashboard.streamlit.app/)

One-click deployment to Streamlit Cloud

</td>
<td width="33%" align="center">

### **ğŸ³ Docker**
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker)](https://docker.com/)

Containerized deployment for any platform

</td>
<td width="33%" align="center">

### **ğŸ”§ Local**
[![Local](https://img.shields.io/badge/Local-Setup-green?style=for-the-badge&logo=python)](https://python.org/)

Full control with local installation

</td>
</tr>
</table>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

1. ğŸ´ **Fork** the repository
2. ğŸŒŸ **Create** a feature branch
3. âœ… **Add** tests for new functionality
4. ğŸš€ **Submit** a pull request

**[Contribution Guidelines](CONTRIBUTING.md)** | **[Code of Conduct](CODE_OF_CONDUCT.md)**

---

## ğŸ“š Documentation

<div align="center">
<table>
<tr>
<td width="25%" align="center">
<a href="#api-documentation">ğŸ“– API Docs</a><br>
<em>Complete API reference</em>
</td>
<td width="25%" align="center">
<a href="#user-guide">ğŸ‘¥ User Guide</a><br>
<em>Step-by-step tutorials</em>
</td>
<td width="25%" align="center">
<a href="#examples">ğŸ’¡ Examples</a><br>
<em>Code samples & demos</em>
</td>
<td width="25%" align="center">
<a href="#faq">â“ FAQ</a><br>
<em>Common questions</em>
</td>
</tr>
</table>
</div>

---

## ğŸ† Recognition

<div align="center">

**â­ Star this repository if you find it useful!**

[![GitHub stars](https://img.shields.io/github/stars/ishabanya/llm_benchmark_dashboard?style=social)](https://github.com/ishabanya/llm_benchmark_dashboard/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/ishabanya/llm_benchmark_dashboard?style=social)](https://github.com/ishabanya/llm_benchmark_dashboard/network)

</div>

---

## ğŸ“ Contact & Support

<div align="center">

**ğŸ‘¨â€ğŸ’» Built by Shabanya Kishore Yadagini**

[![Portfolio](https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white)](https://your-portfolio.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/shabanya-kishore-yadagini-9a7a55249/)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:yadaginishabanya@gmail.com)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ishabanya)

</div>

---

## ğŸ“„ License

<div align="center">

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Built with â¤ï¸ for the AI community**

---

### ğŸŒŸ **[Try the Live Demo Now!](https://llmbenchmarkdashboard.streamlit.app/)**

*Experience the future of LLM evaluation*

</div>
